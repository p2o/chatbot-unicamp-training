{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importação da biblioteca `google.colab.drive` e montagem do Google Drive usando o método `drive.mount('/content/drive')`, no ambiente do Colab para acessar os arquivos e diretórios no drive. É necessário autorizar o acesso ao drive."
      ],
      "metadata": {
        "id": "M0ifm6GehnKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siY_cf_GMZt_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalação das bibliotecas `transformers` e `datasets` usando o comando `!pip install`. As bibliotecas `transformers` fornecem muitos modelos pré-treinados e utilidades para processamento de linguagem natural, enquanto a biblioteca `datasets` contém vários conjuntos de dados para treinamento e avaliação de modelos."
      ],
      "metadata": {
        "id": "aLSKypGEhnoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytVOPAUEoWk9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição da versão do pacote `pytorch-xla` e instalação usando o comando `!curl` para baixar o script de configuração e `!python` para executar o script. Isso é necessário para executar o código em um ambiente distribuído usando o TPU (Unidade de Processamento Tensorial)."
      ],
      "metadata": {
        "id": "9_dYJVNzh0SM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGkoR-bspmvn"
      },
      "outputs": [],
      "source": [
        "VERSION = \"1.8.1\"\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de variáveis, como `squad_v2` (que indica se o conjunto de dados SQuAD é a versão 2), `model_checkpoint` (que especifica o modelo BERT pré-treinado em português a ser usado) e `batch_size` (o tamanho do lote de dados usado durante o treinamento)."
      ],
      "metadata": {
        "id": "Mwig58Vghz8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc_lOQ5ONQKD"
      },
      "outputs": [],
      "source": [
        "squad_v2 = False\n",
        "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação das funções `load_dataset` e `load_metric` da biblioteca `datasets`. Essas funções são usadas para carregar conjuntos de dados e métricas de avaliação para treinar os modelos."
      ],
      "metadata": {
        "id": "ARrXfkGkhzmi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApBLpNGtwf5_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download do conjunto de dados SQuAD em português usando o comando `!wget`. O comando baixa um arquivo .tar.gz do Google Drive e salva-o como squad-pt.tar.gz. A opção --load-cookies é usada para especificar o local de um arquivo de cookies temporário usado para autenticar o download."
      ],
      "metadata": {
        "id": "ijnXjVK6hzS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg4ZCkmmwkIW"
      },
      "outputs": [],
      "source": [
        "# Get dataset SQUAD in Portuguese\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrai o arquivo .tar.gz usando o comando `!tar -xvf squad-pt.tar.gz`. O comando extrai os arquivos do conjunto de dados SQuAD em português."
      ],
      "metadata": {
        "id": "U5GjDW7ohyt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e89bF9L4w38g"
      },
      "outputs": [],
      "source": [
        "!tar -xvf squad-pt.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A célula contém um bloco de código que é executado com o temporizador ativado (`%%time`). Ele é responsável por ler os arquivos JSON do SQuAD em português e convertê-los em um novo formato exigido pelo modelo de perguntas e respostas. A célula itera sobre os arquivos JSON, extrai informações relevantes, como contexto, pergunta e respostas, e as armazena em um novo dicionário. Esse novo arquivo é salvo no disco com um novo nome (prefixado com 'pt_')."
      ],
      "metadata": {
        "id": "jIhZFp6ehydl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjx4_y-3w9YU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# new\n",
        "\n",
        "# Get the train and validation json file in the HF script format\n",
        "# inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
        "\n",
        "import json\n",
        "\n",
        "files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
        "\n",
        "for file in files:\n",
        "\n",
        "    # Opening JSON file & returns JSON object as a dictionary\n",
        "    f = open(file, encoding=\"utf-8\")\n",
        "    data = json.load(f)\n",
        "\n",
        "    # Iterating through the json list\n",
        "    entry_list = list()\n",
        "    id_list = list()\n",
        "\n",
        "    for row in data['data']:\n",
        "        title = row['title']\n",
        "\n",
        "        for paragraph in row['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "\n",
        "            for qa in paragraph['qas']:\n",
        "                entry = {}\n",
        "\n",
        "                qa_id = qa['id']\n",
        "                question = qa['question']\n",
        "                answers = qa['answers']\n",
        "\n",
        "                entry['id'] = qa_id\n",
        "                entry['title'] = title.strip()\n",
        "                entry['context'] = context.strip()\n",
        "                entry['question'] = question.strip()\n",
        "\n",
        "                answer_starts = [answer[\"answer_start\"] for answer in answers]\n",
        "                answer_texts = [answer[\"text\"].strip() for answer in answers]\n",
        "                entry['answers'] = {}\n",
        "                entry['answers']['answer_start'] = answer_starts\n",
        "                entry['answers']['text'] = answer_texts\n",
        "\n",
        "                entry_list.append(entry)\n",
        "\n",
        "    reverse_entry_list = entry_list[::-1]\n",
        "\n",
        "    # for entries with same id, keep only last one (corrected texts by he group Deep Learning Brasil)\n",
        "    unique_ids_list = list()\n",
        "    unique_entry_list = list()\n",
        "    for entry in reverse_entry_list:\n",
        "        qa_id = entry['id']\n",
        "        if qa_id not in unique_ids_list:\n",
        "            unique_ids_list.append(qa_id)\n",
        "            unique_entry_list.append(entry)\n",
        "\n",
        "    # Closing file\n",
        "    f.close()\n",
        "\n",
        "    new_dict = {}\n",
        "    new_dict['data'] = unique_entry_list\n",
        "\n",
        "    file_name = 'pt_' + str(file)\n",
        "    with open(file_name, 'w') as json_file:\n",
        "        json.dump(new_dict, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega os conjuntos de dados de treinamento e validação usando `load_dataset` da biblioteca `datasets`. Os arquivos JSON recém-processados são especificados e os nomes dos campos são definidos para que o pipeline de treinamento as reconheça corretamente."
      ],
      "metadata": {
        "id": "ChkpP8HChx4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWk3S5zxKsR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "datasets = load_dataset('json',\n",
        "                        data_files={'train': 'pt_squad-train-v1.1.json', 'validation': 'pt_squad-dev-v1.1.json'},\n",
        "                        field='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define uma função chamada `show_random_elements` que exibe aleatoriamente algumas entradas do conjunto de dados. A função usa a biblioteca `pandas` para exibir os dados em formato de tabela."
      ],
      "metadata": {
        "id": "1tBijwqwhxoj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr72KP-zxYVV"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostra exemplos aleatórios do conjunto de treinamento usando a função `show_random_elements`."
      ],
      "metadata": {
        "id": "rYAJ6XSjhw9d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me9QqPujxaeX"
      },
      "outputs": [],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação do módulo `AutoTokenizer` da biblioteca `transformers` e criação de um objeto `tokenizer` que será usado para tokenizar os dados de entrada antes de alimentá-los para o modelo. O método `from_pretrained` é usado para carregar o tokenizador pré-treinado correspondente ao modelo BERT especificado."
      ],
      "metadata": {
        "id": "JMgnijcWhwtE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blAhR6gexfpf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição das variáveis `max_length` e `doc_stride`. `max_length` é o comprimento máximo de uma entrada (pergunta e contexto) que será alimentada para o modelo. `doc_stride` é a sobreposição autorizada entre duas partes do contexto ao dividi-lo.\n",
        "\n",
        "Itera sobre o conjunto de dados de treinamento para encontrar o primeiro exemplo cujo comprimento de entrada (pergunta + contexto) excede `max_length`. O exemplo é armazenado na variável `example` para fins de depuração. Em seguida, o comprimento da entrada tokenizada é impresso."
      ],
      "metadata": {
        "id": "zUrujUfdhwdV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BKVmVv002sg"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "\n",
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n",
        "        break\n",
        "example = datasets[\"train\"][i]\n",
        "\n",
        "print(example)\n",
        "\n",
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifica se o tokenizador insere o preenchimento à direita (`right`). Essa informação será usada posteriormente para criar recursos de treinamento."
      ],
      "metadata": {
        "id": "5GcDUvC7hwGO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yehb_rIP2XCL"
      },
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a função `prepare_train_features` que é usada para preparar os exemplos de treinamento. Os exemplos são tokenizados, divididos em vários recursos (se necessário) e as respostas são mapeadas para as posições tokenizadas correspondentes. O resultado é um dicionário com os tokens de entrada e as posições das respostas."
      ],
      "metadata": {
        "id": "EyfDka44hv6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6963hNSf3HVY"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplica a função `prepare_train_features` aos conjuntos de dados de treinamento e validação mapeados. O resultado é armazenado na variável `tokenized_datasets`."
      ],
      "metadata": {
        "id": "ogF7rru9hvCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ixd4mg21Cj"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define o nome de arquivo com base no modelo BERT usado (`model_checkpoint`) e salva o modelo treinado e o tokenizador no diretório `model_trained`."
      ],
      "metadata": {
        "id": "E0su8N1whuz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xayd_Y3mzWDX"
      },
      "outputs": [],
      "source": [
        "file_name = str(model_checkpoint).replace('/','-') + '-ric_finetuned-squad-v1.1-pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importa as bibliotecas `transformers`, `AutoModelForQuestionAnswering`, `TrainingArguments` e `Trainer`. Cria uma instância do modelo BERT pré-treinado usando o método `from_pretrained`. Em seguida, envolve o modelo em um objeto `MpModelWrapper` da biblioteca `torch_xla` para treinamento distribuído usando o TPU."
      ],
      "metadata": {
        "id": "VGhDtefRhuit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9cijpr7kmBZ"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da função `default_data_collator` da biblioteca `transformers`. Criação de um objeto `data_collator` que será usado para agrupar os dados de treinamento durante o treinamento do modelo."
      ],
      "metadata": {
        "id": "z_PPWev-huM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FAAf1ajzwZj"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a função de treinamento `training` que é chamada durante a fase de treinamento com `Trainer`. A função define os argumentos de treinamento, cria uma instância de `Trainer` e treina o modelo. O modelo treinado e o tokenizador são salvos no diretório `model_trained`."
      ],
      "metadata": {
        "id": "84kcrgF1ht-w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoLsFj2lzz3z"
      },
      "outputs": [],
      "source": [
        "def training(model):\n",
        "    args = TrainingArguments(\n",
        "        f\"{file_name}\",\n",
        "        num_train_epochs=4.0,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        save_strategy = \"epoch\",\n",
        "        weight_decay=0.001,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        #save_total_limit=1\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.place_model_on_device = False\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"model_trained/\")\n",
        "    tokenizer.save_pretrained(\"model_trained/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação de código distribuído usando a biblioteca `torch_xla`. A função `_mp_fn` é usada para criar uma instância de modelo em cada processo distribuído usando uma determinada estratégia (`xm.xla_device`). O modelo é treinado chamando a função `training`.\n",
        "\n",
        "Função `main` que chama a função `_mp_fn` para iniciar o treinamento distribuído. O número de processos a serem executados pode ser especificado usando o argumento `nprocs`."
      ],
      "metadata": {
        "id": "TkAsVSMAhtTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTXQV6M4z1Wi"
      },
      "outputs": [],
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def _mp_fn(index):\n",
        "    device = xm.xla_device()\n",
        "\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    training(model)\n",
        "\n",
        "xmp.spawn(_mp_fn, nprocs=8, start_method=\"fork\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código para mover do treinamento local para o Google Drive usando o módulo `pathlib` para manuseio de caminhos de arquivos. O diretório de trabalho atual (`dest/`) é movido para o diretório `model_trained` no Google Drive."
      ],
      "metadata": {
        "id": "Q-cOVtz9hs8c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-_D_oPeJBKy"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "dest = pathlib.Path.cwd()\n",
        "fname_HF = 'model_trained'\n",
        "\n",
        "path_to_model = dest/fname_HF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação do módulo `Pipeline` da biblioteca `transformers`. Criação de um objeto `nlp` para responder a perguntas usando o modelo treinado. O modelo treinado é especificado no argumento `model`."
      ],
      "metadata": {
        "id": "-kAanPDjhspC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALKekJ0qFiZl"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_qa = str(path_to_model)\n",
        "nlp = pipeline(\"question-answering\", model=model_qa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de um contexto no qual uma pergunta será feita. O contexto é uma passagem de texto em que a pergunta será respondida."
      ],
      "metadata": {
        "id": "j_AHIE1yhr7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxMRoRKZHh4Z"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Integralização de um curso é quando o estudante cumpre todas as exigências fixadas no Catálogos de cursos.\n",
        "O sistema de integralização adotado pela Universidade é o de crédito e matrícula por disciplina.\n",
        "O relatório de Integralização Curricular é publicado na web semestralmente, constando a relação de disciplinas cursadas com suas respectivas informações - se é obrigatória, eletiva ou extracurricular, créditos e nota obtida -, as disciplinas em que o estudante está matriculado atualmente, as disciplinas que ainda precisam ser cursadas e o prazo máximo para conclusão do curso, dentre outras informações.\n",
        "O estudante pode consultar esse documento no site da DAC, em Integralização Curricular, após logar em Serviços Acadêmicos.\n",
        "Prazo mínimo e máximo são os tempos mínimo e máximo que o estudante dispõe para a conclusão do curso. O tempo mínimo é igual ao número de períodos fixados pelas Unidades de Ensino na proposta para o cumprimento do Currículo Pleno. O tempo máximo corresponde à soma do tempo mínimo e mais 50% desse período (exemplo: para um curso de 08 semestres de período mínimo, o prazo máximo seria de 12 semestres).\n",
        "Do tempo máximo de integralização deve ser subtraído o tempo referente a aproveitamento de estudos, dispensas de disciplinas e teste de proficiência. Isso quer dizer que o período máximo pode ser menor do que o estipulado inicialmente quando o estudante solicitar aproveitamento e/ou teste de proficiência.\n",
        "Caso o estudante solicite trancamento de semestre, o prazo máximo de integralização será acrescido do mesmo número de períodos letivos em que a matrícula esteve trancada. Isso quer dizer que o período máximo será maior que o estipulado inicialmente quando o estudante solicitar trancamento de semestre.\n",
        "O estudante pode verificar seu prazo de integralização no site da DAC, em Integralização Curricular, após logar em Serviços Acadêmicos.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de uma pergunta que será feita sobre o contexto. A pergunta é especificada no argumento `question` ao chamar o objeto `nlp` para obter a resposta.\n",
        "\n",
        "Impressão da resposta obtida pelo modelo, a pontuação atribuída à resposta e as posições de início e fim da resposta no contexto original."
      ],
      "metadata": {
        "id": "5e_oKoJhhrnZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG38XKIUK6R5"
      },
      "outputs": [],
      "source": [
        "question = \"O que tem no relatório de integralização?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de um contexto no qual uma pergunta será feita. O contexto é uma passagem de texto em que a pergunta será respondida."
      ],
      "metadata": {
        "id": "1LlttUrnhrVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWvCjb00GQfM"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Certificado de Estudos é o documento que atesta a conclusão de um elenco de disciplinas integrantes de um ramo específico do conhecimento.\n",
        "O oferecimento de Certificados de Estudos bem como a definição do elenco de disciplinas que os compõem ficam a critério da Coordenadoria de Curso responsável pelas disciplinas envolvidas.\n",
        "Os Certificados de Estudos são emitidos pela Diretoria Acadêmica (DAC) quando solicitados pelos estudantes regulares ou especiais que forem aprovados em todas as disciplinas exigidas.\n",
        "Mais informações podem ser obtidas no Regimento Geral dos cursos de Graduação.\n",
        "Procedimentos: Para solicitar o Certificado de Estudos, o estudante deve protocolar seu pedido pelo sistema e-DAC, utilizando o assunto 'Certificado de Estudos' e citando, no campo de texto, o nome do certificado, o catalogo do qual o certificado faz parte(curso/ano) e as disciplinas foram cursadas para este fim.\n",
        "Caso seja necessário, a Diretoria Acadêmica(DAC) irá solicitar que a Coordenadoria de Graduação responsável emita um ofício, validando o cumprimento das exigidas. Constatando o cumprimento das exigências, a DAC emitirá o Certificado e avisará quando o mesmo estiver disponível para a retirada.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de uma pergunta que será feita sobre o contexto. A pergunta é especificada no argumento `question` ao chamar o objeto `nlp` para obter a resposta.\n",
        "\n",
        "Impressão da resposta obtida pelo modelo, a pontuação atribuída à resposta e as posições de início e fim da resposta no contexto original."
      ],
      "metadata": {
        "id": "HqE-e4NHhqjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdFrf8XDMnGh"
      },
      "outputs": [],
      "source": [
        "question = \"Como faço para pedir um certificado de estudos?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de um contexto no qual uma pergunta será feita. O contexto é uma passagem de texto em que a pergunta será respondida."
      ],
      "metadata": {
        "id": "3-VdtwD-hqUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxF6PtqgHBnF"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Para efeito de cálculo de carga horária, serão atribuídas a cada disciplina unidades de crédito, sendo que cada unidade de crédito corresponde a 15 (quinze) horas/aula, com duração de 15 (quinze) semanas, para as disciplinas semestrais.\n",
        "O total de créditos a ser cumprido pelo estudante para conclusão do curso será estabelecido pelo Catálogo dos Cursos de Graduação.\n",
        "Pré-requisitos são condições consideradas indispensáveis para matrícula em disciplinas, podendo ser classificado como:\n",
        "Pré-requisito pleno: disciplina ou o conjunto de disciplinas em que o estudante deve obter aprovação para matricular-se em outra disciplina.\n",
        "Pré-requisito parcial: a disciplina ou o conjunto de disciplinas em que o estudante deve obter frequência mínima e média final maior ou igual a 3,0 (três) para matricular-se em outra disciplina.\n",
        "Pré-requisito especial: condição não atrelada a disciplinas específicas, podendo ser AA4nn - (pela exigência de Coeficiente de Progressão maior ou igual a 0,xx) ou AA200 (por autorização da Coordenadoria que a oferece).\n",
        "Não é autorizada a utilização da AA200 para disciplinas obrigatórias, devendo os casos excepcionais ser devidamente justificados e remetidos à CCG para apreciação Todos os pré-requisitos exigidos para as disciplinas do curso de graduação constam no Currículo Pleno seguido pelo estudante.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definição de uma pergunta que será feita sobre o contexto. A pergunta é especificada no argumento `question` ao chamar o objeto `nlp` para obter a resposta.\n",
        "\n",
        "Impressão da resposta obtida pelo modelo, a pontuação atribuída à resposta e as posições de início e fim da resposta no contexto original."
      ],
      "metadata": {
        "id": "QqV1auP7hpvz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxp-qOKdWIoT"
      },
      "outputs": [],
      "source": [
        "question = \"O que é um pré requisito parcial?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria uma nova pasta chamada `model_trained_v30` no Google Drive e copia o conteúdo do diretório `model_trained` para a nova pasta."
      ],
      "metadata": {
        "id": "AGnimmPehpbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AY2f7Z-G4-I"
      },
      "outputs": [],
      "source": [
        "!cp -r model_trained/ drive/MyDrive/model_trained_v30/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}