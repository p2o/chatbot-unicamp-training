{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siY_cf_GMZt_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytVOPAUEoWk9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGkoR-bspmvn"
      },
      "outputs": [],
      "source": [
        "VERSION = \"1.8.1\"\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc_lOQ5ONQKD"
      },
      "outputs": [],
      "source": [
        "squad_v2 = False\n",
        "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApBLpNGtwf5_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg4ZCkmmwkIW"
      },
      "outputs": [],
      "source": [
        "# Get dataset SQUAD in Portuguese\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Q0IaIlv2h2BC468MwUFmUST0EyN7gNkn\" -O squad-pt.tar.gz && rm -rf /tmp/cookies.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e89bF9L4w38g"
      },
      "outputs": [],
      "source": [
        "!tar -xvf squad-pt.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjx4_y-3w9YU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# new\n",
        "\n",
        "# Get the train and validation json file in the HF script format\n",
        "# inspiration: file squad.py at https://github.com/huggingface/datasets/tree/master/datasets/squad\n",
        "\n",
        "import json\n",
        "\n",
        "files = ['squad-train-v1.1.json','squad-dev-v1.1.json']\n",
        "\n",
        "for file in files:\n",
        "\n",
        "    # Opening JSON file & returns JSON object as a dictionary\n",
        "    f = open(file, encoding=\"utf-8\")\n",
        "    data = json.load(f)\n",
        "\n",
        "    # Iterating through the json list\n",
        "    entry_list = list()\n",
        "    id_list = list()\n",
        "\n",
        "    for row in data['data']:\n",
        "        title = row['title']\n",
        "\n",
        "        for paragraph in row['paragraphs']:\n",
        "            context = paragraph['context']\n",
        "\n",
        "            for qa in paragraph['qas']:\n",
        "                entry = {}\n",
        "\n",
        "                qa_id = qa['id']\n",
        "                question = qa['question']\n",
        "                answers = qa['answers']\n",
        "\n",
        "                entry['id'] = qa_id\n",
        "                entry['title'] = title.strip()\n",
        "                entry['context'] = context.strip()\n",
        "                entry['question'] = question.strip()\n",
        "\n",
        "                answer_starts = [answer[\"answer_start\"] for answer in answers]\n",
        "                answer_texts = [answer[\"text\"].strip() for answer in answers]\n",
        "                entry['answers'] = {}\n",
        "                entry['answers']['answer_start'] = answer_starts\n",
        "                entry['answers']['text'] = answer_texts\n",
        "\n",
        "                entry_list.append(entry)\n",
        "\n",
        "    reverse_entry_list = entry_list[::-1]\n",
        "\n",
        "    # for entries with same id, keep only last one (corrected texts by he group Deep Learning Brasil)\n",
        "    unique_ids_list = list()\n",
        "    unique_entry_list = list()\n",
        "    for entry in reverse_entry_list:\n",
        "        qa_id = entry['id']\n",
        "        if qa_id not in unique_ids_list:\n",
        "            unique_ids_list.append(qa_id)\n",
        "            unique_entry_list.append(entry)\n",
        "\n",
        "    # Closing file\n",
        "    f.close()\n",
        "\n",
        "    new_dict = {}\n",
        "    new_dict['data'] = unique_entry_list\n",
        "\n",
        "    file_name = 'pt_' + str(file)\n",
        "    with open(file_name, 'w') as json_file:\n",
        "        json.dump(new_dict, json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWk3S5zxKsR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "datasets = load_dataset('json',\n",
        "                        data_files={'train': 'pt_squad-train-v1.1.json', 'validation': 'pt_squad-dev-v1.1.json'},\n",
        "                        field='data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr72KP-zxYVV"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me9QqPujxaeX"
      },
      "outputs": [],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blAhR6gexfpf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BKVmVv002sg"
      },
      "outputs": [],
      "source": [
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
        "\n",
        "for i, example in enumerate(datasets[\"train\"]):\n",
        "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n",
        "        break\n",
        "example = datasets[\"train\"][i]\n",
        "\n",
        "print(example)\n",
        "\n",
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yehb_rIP2XCL"
      },
      "outputs": [],
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6963hNSf3HVY"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2ixd4mg21Cj"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xayd_Y3mzWDX"
      },
      "outputs": [],
      "source": [
        "file_name = str(model_checkpoint).replace('/','-') + '-ric_finetuned-squad-v1.1-pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9cijpr7kmBZ"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FAAf1ajzwZj"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoLsFj2lzz3z"
      },
      "outputs": [],
      "source": [
        "def training(model):\n",
        "    args = TrainingArguments(\n",
        "        f\"{file_name}\",\n",
        "        num_train_epochs=4.0,\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        save_strategy = \"epoch\",\n",
        "        weight_decay=0.001,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        #save_total_limit=1\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    trainer.place_model_on_device = False\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"model_trained/\")\n",
        "    tokenizer.save_pretrained(\"model_trained/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTXQV6M4z1Wi"
      },
      "outputs": [],
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def _mp_fn(index):\n",
        "    device = xm.xla_device()\n",
        "\n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    training(model)\n",
        "\n",
        "xmp.spawn(_mp_fn, nprocs=8, start_method=\"fork\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-_D_oPeJBKy"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "dest = pathlib.Path.cwd()\n",
        "fname_HF = 'model_trained'\n",
        "\n",
        "path_to_model = dest/fname_HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALKekJ0qFiZl"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_qa = str(path_to_model)\n",
        "nlp = pipeline(\"question-answering\", model=model_qa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxMRoRKZHh4Z"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Integralização de um curso é quando o estudante cumpre todas as exigências fixadas no Catálogos de cursos.\n",
        "O sistema de integralização adotado pela Universidade é o de crédito e matrícula por disciplina.\n",
        "O relatório de Integralização Curricular é publicado na web semestralmente, constando a relação de disciplinas cursadas com suas respectivas informações - se é obrigatória, eletiva ou extracurricular, créditos e nota obtida -, as disciplinas em que o estudante está matriculado atualmente, as disciplinas que ainda precisam ser cursadas e o prazo máximo para conclusão do curso, dentre outras informações.\n",
        "O estudante pode consultar esse documento no site da DAC, em Integralização Curricular, após logar em Serviços Acadêmicos.\n",
        "Prazo mínimo e máximo são os tempos mínimo e máximo que o estudante dispõe para a conclusão do curso. O tempo mínimo é igual ao número de períodos fixados pelas Unidades de Ensino na proposta para o cumprimento do Currículo Pleno. O tempo máximo corresponde à soma do tempo mínimo e mais 50% desse período (exemplo: para um curso de 08 semestres de período mínimo, o prazo máximo seria de 12 semestres).\n",
        "Do tempo máximo de integralização deve ser subtraído o tempo referente a aproveitamento de estudos, dispensas de disciplinas e teste de proficiência. Isso quer dizer que o período máximo pode ser menor do que o estipulado inicialmente quando o estudante solicitar aproveitamento e/ou teste de proficiência.\n",
        "Caso o estudante solicite trancamento de semestre, o prazo máximo de integralização será acrescido do mesmo número de períodos letivos em que a matrícula esteve trancada. Isso quer dizer que o período máximo será maior que o estipulado inicialmente quando o estudante solicitar trancamento de semestre.\n",
        "O estudante pode verificar seu prazo de integralização no site da DAC, em Integralização Curricular, após logar em Serviços Acadêmicos.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG38XKIUK6R5"
      },
      "outputs": [],
      "source": [
        "question = \"O que tem no relatório de integralização?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWvCjb00GQfM"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Certificado de Estudos é o documento que atesta a conclusão de um elenco de disciplinas integrantes de um ramo específico do conhecimento.\n",
        "O oferecimento de Certificados de Estudos bem como a definição do elenco de disciplinas que os compõem ficam a critério da Coordenadoria de Curso responsável pelas disciplinas envolvidas.\n",
        "Os Certificados de Estudos são emitidos pela Diretoria Acadêmica (DAC) quando solicitados pelos estudantes regulares ou especiais que forem aprovados em todas as disciplinas exigidas.\n",
        "Mais informações podem ser obtidas no Regimento Geral dos cursos de Graduação.\n",
        "Procedimentos: Para solicitar o Certificado de Estudos, o estudante deve protocolar seu pedido pelo sistema e-DAC, utilizando o assunto 'Certificado de Estudos' e citando, no campo de texto, o nome do certificado, o catalogo do qual o certificado faz parte(curso/ano) e as disciplinas foram cursadas para este fim.\n",
        "Caso seja necessário, a Diretoria Acadêmica(DAC) irá solicitar que a Coordenadoria de Graduação responsável emita um ofício, validando o cumprimento das exigidas. Constatando o cumprimento das exigências, a DAC emitirá o Certificado e avisará quando o mesmo estiver disponível para a retirada.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdFrf8XDMnGh"
      },
      "outputs": [],
      "source": [
        "question = \"Como faço para pedir um certificado de estudos?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxF6PtqgHBnF"
      },
      "outputs": [],
      "source": [
        "context = r\"\"\"\n",
        "Para efeito de cálculo de carga horária, serão atribuídas a cada disciplina unidades de crédito, sendo que cada unidade de crédito corresponde a 15 (quinze) horas/aula, com duração de 15 (quinze) semanas, para as disciplinas semestrais.\n",
        "O total de créditos a ser cumprido pelo estudante para conclusão do curso será estabelecido pelo Catálogo dos Cursos de Graduação.\n",
        "Pré-requisitos são condições consideradas indispensáveis para matrícula em disciplinas, podendo ser classificado como:\n",
        "Pré-requisito pleno: disciplina ou o conjunto de disciplinas em que o estudante deve obter aprovação para matricular-se em outra disciplina.\n",
        "Pré-requisito parcial: a disciplina ou o conjunto de disciplinas em que o estudante deve obter frequência mínima e média final maior ou igual a 3,0 (três) para matricular-se em outra disciplina.\n",
        "Pré-requisito especial: condição não atrelada a disciplinas específicas, podendo ser AA4nn - (pela exigência de Coeficiente de Progressão maior ou igual a 0,xx) ou AA200 (por autorização da Coordenadoria que a oferece).\n",
        "Não é autorizada a utilização da AA200 para disciplinas obrigatórias, devendo os casos excepcionais ser devidamente justificados e remetidos à CCG para apreciação Todos os pré-requisitos exigidos para as disciplinas do curso de graduação constam no Currículo Pleno seguido pelo estudante.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxp-qOKdWIoT"
      },
      "outputs": [],
      "source": [
        "question = \"O que é um pré requisito parcial?\"\n",
        "\n",
        "result = nlp(question=question, context=context)\n",
        "\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AY2f7Z-G4-I"
      },
      "outputs": [],
      "source": [
        "!cp -r model_trained/ drive/MyDrive/model_trained_v30/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}